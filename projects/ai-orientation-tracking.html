<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI-Based 3D Orientation Tracking | Hassan Nader</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <style>
        :root {
            --primary-color: #2563eb;
            --secondary-color: #1e40af;
            --accent-color: #3b82f6;
            --dark-bg: #0f172a;
            --card-bg: #1e293b;
            --text-primary: #f1f5f9;
            --text-secondary: #cbd5e1;
            --text-muted: #94a3b8;
            --border-color: #334155;
            --success-color: #10b981;
            --gradient-start: #1e40af;
            --gradient-end: #7c3aed;
        }

        [data-theme="light"] {
            --primary-color: #2563eb;
            --secondary-color: #1e40af;
            --accent-color: #3b82f6;
            --dark-bg: #ffffff;
            --card-bg: #f8fafc;
            --text-primary: #0f172a;
            --text-secondary: #475569;
            --text-muted: #64748b;
            --border-color: #e2e8f0;
            --success-color: #10b981;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', sans-serif;
            background: var(--dark-bg);
            color: var(--text-primary);
            line-height: 1.6;
            transition: background-color 0.3s ease, color 0.3s ease;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        /* Navigation */
        nav {
            position: fixed;
            top: 0;
            width: 100%;
            background: rgba(15, 23, 42, 0.95);
            backdrop-filter: blur(10px);
            z-index: 1000;
            padding: 1rem 0;
            border-bottom: 1px solid var(--border-color);
        }

        nav .container {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .back-btn {
            color: var(--accent-color);
            text-decoration: none;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            font-weight: 600;
            transition: gap 0.3s;
        }

        .back-btn:hover {
            gap: 1rem;
        }

        .theme-toggle {
            position: relative;
            width: 60px;
            height: 30px;
            background: var(--border-color);
            border-radius: 15px;
            cursor: pointer;
            transition: background 0.3s ease;
            border: none;
        }

        .theme-toggle::before {
            content: '';
            position: absolute;
            width: 24px;
            height: 24px;
            border-radius: 50%;
            background: white;
            top: 3px;
            left: 3px;
            transition: transform 0.3s ease;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
        }

        [data-theme="light"] .theme-toggle {
            background: var(--accent-color);
        }

        [data-theme="light"] .theme-toggle::before {
            transform: translateX(30px);
        }

        .theme-toggle-icon {
            position: absolute;
            top: 50%;
            transform: translateY(-50%);
            font-size: 0.8rem;
            transition: opacity 0.3s ease;
        }

        .theme-toggle .sun-icon {
            right: 8px;
            color: #fbbf24;
            opacity: 0;
        }

        .theme-toggle .moon-icon {
            left: 8px;
            color: #cbd5e1;
            opacity: 1;
        }

        [data-theme="light"] .theme-toggle .sun-icon {
            opacity: 1;
        }

        [data-theme="light"] .theme-toggle .moon-icon {
            opacity: 0;
        }

        /* Hero Section */
        .project-hero {
            padding: 120px 0 60px;
            background: linear-gradient(135deg, var(--gradient-start) 0%, var(--gradient-end) 100%);
            position: relative;
            overflow: hidden;
        }

        .project-hero::before {
            content: '';
            position: absolute;
            width: 200%;
            height: 200%;
            background: radial-gradient(circle, rgba(59, 130, 246, 0.1) 1px, transparent 1px);
            background-size: 50px 50px;
            animation: float 20s linear infinite;
        }

        @keyframes float {
            0% { transform: translate(0, 0); }
            100% { transform: translate(50px, 50px); }
        }

        .hero-content {
            position: relative;
            z-index: 1;
        }

        .project-icon {
            font-size: 4rem;
            color: white;
            margin-bottom: 1rem;
        }

        .project-hero h1 {
            font-size: 3rem;
            color: white;
            margin-bottom: 1rem;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
        }

        .project-meta {
            display: flex;
            gap: 2rem;
            flex-wrap: wrap;
            margin-top: 1.5rem;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            color: rgba(255, 255, 255, 0.9);
            font-size: 1rem;
        }

        .meta-item i {
            color: rgba(255, 255, 255, 0.7);
        }

        /* Action Buttons */
        .action-buttons {
            display: flex;
            gap: 1rem;
            margin-top: 2rem;
            flex-wrap: wrap;
        }

        .btn {
            padding: 0.875rem 2rem;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 600;
            transition: all 0.3s;
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            border: none;
            cursor: pointer;
        }

        .btn-primary {
            background: white;
            color: var(--primary-color);
        }

        .btn-primary:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 25px rgba(255, 255, 255, 0.2);
        }

        .btn-secondary {
            background: rgba(255, 255, 255, 0.1);
            color: white;
            border: 2px solid white;
        }

        .btn-secondary:hover {
            background: white;
            color: var(--primary-color);
        }

        /* Content Sections */
        .content-section {
            padding: 4rem 0;
        }

        .section-title {
            font-size: 2rem;
            margin-bottom: 2rem;
            color: var(--text-primary);
            display: flex;
            align-items: center;
            gap: 0.75rem;
        }

        .section-title i {
            color: var(--accent-color);
        }

        /* Overview Grid */
        .overview-grid {
            display: grid;
            grid-template-columns: 2fr 1fr;
            gap: 2rem;
            margin-bottom: 3rem;
        }

        .overview-text {
            font-size: 1.1rem;
            color: var(--text-secondary);
            line-height: 1.8;
        }

        .overview-text p {
            margin-bottom: 1.5rem;
        }

        .highlight {
            color: var(--accent-color);
            font-weight: 600;
        }

        /* Stats Card */
        .stats-card {
            background: var(--card-bg);
            padding: 2rem;
            border-radius: 12px;
            border: 1px solid var(--border-color);
        }

        .stat-item {
            margin-bottom: 1.5rem;
        }

        .stat-item:last-child {
            margin-bottom: 0;
        }

        .stat-label {
            font-size: 0.85rem;
            color: var(--text-muted);
            margin-bottom: 0.5rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .stat-value {
            font-size: 1.75rem;
            font-weight: 700;
            color: var(--accent-color);
        }

        /* Tech Stack */
        .tech-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin-bottom: 3rem;
        }

        .tech-card {
            background: var(--card-bg);
            padding: 1.5rem;
            border-radius: 12px;
            border: 1px solid var(--border-color);
            transition: all 0.3s;
        }

        .tech-card:hover {
            transform: translateY(-5px);
            border-color: var(--accent-color);
            box-shadow: 0 10px 30px rgba(37, 99, 235, 0.2);
        }

        .tech-card h4 {
            color: var(--accent-color);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .tech-card ul {
            list-style: none;
            color: var(--text-secondary);
        }

        .tech-card li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
        }

        .tech-card li::before {
            content: 'â–¹';
            position: absolute;
            left: 0;
            color: var(--accent-color);
        }

        /* Features List */
        .features-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin-bottom: 3rem;
        }

        .feature-card {
            background: var(--card-bg);
            padding: 2rem;
            border-radius: 12px;
            border: 1px solid var(--border-color);
        }

        .feature-icon {
            width: 50px;
            height: 50px;
            background: rgba(59, 130, 246, 0.1);
            border-radius: 10px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.5rem;
            color: var(--accent-color);
            margin-bottom: 1rem;
        }

        .feature-card h3 {
            color: var(--text-primary);
            margin-bottom: 1rem;
            font-size: 1.25rem;
        }

        .feature-card p {
            color: var(--text-secondary);
            line-height: 1.7;
        }

        /* Methodology Timeline */
        .methodology-timeline {
            position: relative;
            padding-left: 2rem;
            margin-bottom: 3rem;
        }

        .methodology-timeline::before {
            content: '';
            position: absolute;
            left: 0;
            top: 0;
            bottom: 0;
            width: 2px;
            background: linear-gradient(180deg, var(--primary-color), var(--accent-color));
        }

        .timeline-step {
            position: relative;
            margin-bottom: 2rem;
            padding-left: 2rem;
        }

        .timeline-step::before {
            content: '';
            position: absolute;
            left: -2.5rem;
            top: 0;
            width: 12px;
            height: 12px;
            border-radius: 50%;
            background: var(--accent-color);
            border: 3px solid var(--dark-bg);
        }

        .step-number {
            display: inline-block;
            background: var(--accent-color);
            color: white;
            width: 30px;
            height: 30px;
            border-radius: 50%;
            text-align: center;
            line-height: 30px;
            font-weight: 700;
            margin-bottom: 0.5rem;
        }

        .timeline-step h4 {
            color: var(--text-primary);
            margin-bottom: 0.5rem;
        }

        .timeline-step p {
            color: var(--text-secondary);
            line-height: 1.7;
        }

        /* Results Section */
        .results-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin-bottom: 3rem;
        }

        .result-card {
            background: linear-gradient(135deg, rgba(37, 99, 235, 0.1), rgba(124, 58, 237, 0.1));
            padding: 2rem;
            border-radius: 12px;
            border: 1px solid var(--border-color);
            text-align: center;
        }

        .result-number {
            font-size: 2.5rem;
            font-weight: 700;
            color: var(--accent-color);
            margin-bottom: 0.5rem;
        }

        .result-label {
            color: var(--text-secondary);
            font-size: 0.95rem;
        }

        /* Code Block */
        .code-section {
            background: var(--card-bg);
            padding: 2rem;
            border-radius: 12px;
            border: 1px solid var(--border-color);
            margin-bottom: 3rem;
        }

        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1rem;
            padding-bottom: 1rem;
            border-bottom: 1px solid var(--border-color);
        }

        .code-title {
            color: var(--text-primary);
            font-weight: 600;
        }

        .copy-btn {
            background: var(--accent-color);
            color: white;
            border: none;
            padding: 0.5rem 1rem;
            border-radius: 6px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: all 0.3s;
        }

        .copy-btn:hover {
            background: var(--secondary-color);
        }

        pre {
            background: rgba(0, 0, 0, 0.3);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 0;
        }

        code {
            color: #a8dadc;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
        }

        /* Challenges Section */
        .challenges-list {
            background: var(--card-bg);
            padding: 2rem;
            border-radius: 12px;
            border: 1px solid var(--border-color);
            margin-bottom: 3rem;
        }

        .challenge-item {
            margin-bottom: 2rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--border-color);
        }

        .challenge-item:last-child {
            margin-bottom: 0;
            padding-bottom: 0;
            border-bottom: none;
        }

        .challenge-title {
            color: #ef4444;
            font-weight: 600;
            margin-bottom: 0.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .solution-title {
            color: var(--success-color);
            font-weight: 600;
            margin-bottom: 0.5rem;
            margin-top: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .challenge-item p {
            color: var(--text-secondary);
            line-height: 1.7;
        }

        /* Image Gallery */
        .image-gallery {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 1.5rem;
            margin-bottom: 3rem;
        }

        .gallery-item {
            background: var(--card-bg);
            border-radius: 12px;
            overflow: hidden;
            border: 1px solid var(--border-color);
        }

        .gallery-image {
            width: 100%;
            height: 250px;
            background: linear-gradient(135deg, var(--primary-color), var(--accent-color));
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 3rem;
            color: white;
        }

        .gallery-caption {
            padding: 1rem;
            color: var(--text-secondary);
            font-size: 0.9rem;
            text-align: center;
        }

        /* Footer */
        footer {
            background: var(--card-bg);
            padding: 2rem 0;
            text-align: center;
            border-top: 1px solid var(--border-color);
            margin-top: 4rem;
        }

        footer p {
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .project-hero h1 {
                font-size: 2rem;
            }

            .overview-grid {
                grid-template-columns: 1fr;
            }

            .tech-grid {
                grid-template-columns: 1fr;
            }

            .features-grid {
                grid-template-columns: 1fr;
            }

            .results-grid {
                grid-template-columns: 1fr;
            }

            .project-meta {
                flex-direction: column;
                gap: 1rem;
            }
        }
    </style>
</head>
<body>
    <nav>
        <div class="container">
            <a href="../index.html" class="back-btn">
                <i class="fas fa-arrow-left"></i> Back to Portfolio
            </a>
            <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                <i class="fas fa-moon theme-toggle-icon moon-icon"></i>
                <i class="fas fa-sun theme-toggle-icon sun-icon"></i>
            </button>
        </div>
    </nav>

    <section class="project-hero">
        <div class="container">
            <div class="hero-content">
                <div class="project-icon">
                    <i class="fas fa-robot"></i>
                </div>
                <h1>AI-Based 3D Orientation Tracking</h1>
                <p style="color: rgba(255, 255, 255, 0.9); font-size: 1.2rem; max-width: 800px;">
                    Revolutionary AI filter system replacing traditional Kalman filters with deep learning neural networks 
                    for superior IMU data processing and real-time 3D orientation estimation.
                </p>
                <div class="project-meta">
                    <div class="meta-item">
                        <i class="fas fa-calendar"></i>
                        <span>Completed: July 2025</span>
                    </div>
                    <div class="meta-item">
                        <i class="fas fa-clock"></i>
                        <span>Duration: 18 months</span>
                    </div>
                    <div class="meta-item">
                        <i class="fas fa-user"></i>
                        <span>Role: Lead Data Scientist & ML Engineer</span>
                    </div>
                </div>
                <div class="action-buttons">
                    <a href="#" class="btn btn-secondary" style="cursor: not-allowed; opacity: 0.7;">
                        <i class="fas fa-file-pdf"></i> Paper Coming Soon
                    </a>
                </div>
            </div>
        </div>
    </section>

    <div class="container">
        <!-- Overview Section -->
        <section class="content-section">
            <h2 class="section-title">
                <i class="fas fa-info-circle"></i>
                Project Overview
            </h2>
            <div class="overview-grid">
                <div class="overview-text">
                    <p>
                        This project presents a <span class="highlight">novel approach to 3D orientation tracking</span> by 
                        replacing traditional Kalman filters with advanced deep learning architectures. The system eliminates 
                        gyroscope dependency by processing raw accelerometer and magnetometer data through LSTM and GRU 
                        neural networks, achieving unprecedented accuracy in real-time orientation estimation through 
                        sophisticated data science and engineering techniques.
                    </p>
                    <p>
                        Traditional Kalman filters, while widely used in robotics and aerospace, rely on linear assumptions 
                        and require careful manual tuning. Our AI-based approach <span class="highlight">learns complex 
                        non-linear patterns</span> directly from sensor data through extensive data preprocessing, feature 
                        engineering, and advanced analytics. The system adapts to various motion conditions and sensor 
                        characteristics without manual intervention, representing a paradigm shift in sensor fusion methodology.
                    </p>
                    <p>
                        Through rigorous data collection, cleaning, and preprocessing of over <span class="highlight">300,000 
                        IMU samples</span>, the system achieved remarkable precision with a mean prediction error of just 0.2Â° 
                        while maintaining real-time performance. The comprehensive data pipeline includes noise filtering, 
                        signal processing, electromagnetic interference mitigation, and synchronized dual-sensor data fusion, 
                        demonstrating the critical role of data engineering in machine learning success.
                    </p>
                </div>
                <div class="stats-card">
                    <div class="stat-item">
                        <div class="stat-label">Mean Error</div>
                        <div class="stat-value">0.2Â°</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-label">Inference Time</div>
                        <div class="stat-value">&lt;100ms</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-label">Data Samples</div>
                        <div class="stat-value">300K+</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-label">Model Accuracy</div>
                        <div class="stat-value">98.7%</div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Tech Stack Section -->
        <section class="content-section">
            <h2 class="section-title">
                <i class="fas fa-layer-group"></i>
                Technology Stack
            </h2>
            <div class="tech-grid">
                <div class="tech-card">
                    <h4><i class="fas fa-brain"></i> ML/DL Frameworks</h4>
                    <ul>
                        <li>TensorFlow 2.x</li>
                        <li>Keras API</li>
                        <li>LSTM Networks</li>
                        <li>GRU Networks</li>
                        <li>CNN Architectures</li>
                        <li>Feedforward Neural Networks</li>
                    </ul>
                </div>
                <div class="tech-card">
                    <h4><i class="fas fa-code"></i> Programming</h4>
                    <ul>
                        <li>Python 3.9</li>
                        <li>NumPy</li>
                        <li>Pandas</li>
                        <li>Scikit-learn</li>
                        <li>Matplotlib</li>
                    </ul>
                </div>
                <div class="tech-card">
                    <h4><i class="fas fa-microchip"></i> Hardware</h4>
                    <ul>
                        <li>IMU Sensors (MPU-9250)</li>
                        <li>ESP32 Microcontroller</li>
                        <li>Serial Communication</li>
                        <li>Real-time Data Streaming</li>
                    </ul>
                </div>
                <div class="tech-card">
                    <h4><i class="fas fa-chart-line"></i> Data Processing</h4>
                    <ul>
                        <li>Signal Processing</li>
                        <li>Data Cleaning & Preprocessing</li>
                        <li>Quaternion Mathematics</li>
                        <li>Sensor Fusion</li>
                        <li>Noise Filtering</li>
                        <li>Feature Engineering</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- Key Features -->
        <section class="content-section">
            <h2 class="section-title">
                <i class="fas fa-star"></i>
                Key Features
            </h2>
            <div class="features-grid">
                <div class="feature-card">
                    <div class="feature-icon">
                        <i class="fas fa-brain"></i>
                    </div>
                    <h3>Deep Learning Architecture</h3>
                    <p>
                        Implemented hybrid LSTM-GRU neural networks that automatically learn temporal dependencies 
                        in sensor data, eliminating the need for manual parameter tuning required by traditional methods.
                    </p>
                </div>
                <div class="feature-card">
                    <div class="feature-icon">
                        <i class="fas fa-bolt"></i>
                    </div>
                    <h3>Real-Time Processing</h3>
                    <p>
                        Optimized inference pipeline achieving sub-100ms latency, making the system suitable for 
                        real-time applications in robotics, VR/AR, and autonomous navigation systems.
                    </p>
                </div>
                <div class="feature-card">
                    <div class="feature-icon">
                        <i class="fas fa-crosshairs"></i>
                    </div>
                    <h3>Superior Accuracy</h3>
                    <p>
                        Achieved 0.2Â° mean prediction error, outperforming traditional Kalman filters by 40% 
                        in challenging motion scenarios with rapid acceleration changes.
                    </p>
                </div>
                <div class="feature-card">
                    <div class="feature-icon">
                        <i class="fas fa-sync-alt"></i>
                    </div>
                    <h3>Adaptive Learning</h3>
                    <p>
                        Model adapts to different sensor characteristics and motion patterns without recalibration, 
                        providing robust performance across various operational conditions.
                    </p>
                </div>
                <div class="feature-card">
                    <div class="feature-icon">
                        <i class="fas fa-database"></i>
                    </div>
                    <h3>Comprehensive Dataset</h3>
                    <p>
                        Trained on 300,000+ carefully curated IMU samples covering diverse motion patterns, 
                        ensuring model generalization and reliability.
                    </p>
                </div>
                <div class="feature-card">
                    <div class="feature-icon">
                        <i class="fas fa-shield-alt"></i>
                    </div>
                    <h3>Noise Robustness</h3>
                    <p>
                        Advanced preprocessing and data augmentation techniques ensure consistent performance 
                        even in high-noise environments typical of real-world applications.
                    </p>
                </div>
            </div>
        </section>

        <!-- Methodology -->
        <section class="content-section">
            <h2 class="section-title">
                <i class="fas fa-project-diagram"></i>
                Methodology
            </h2>
            <div class="methodology-timeline">
                <div class="timeline-step">
                    <span class="step-number">1</span>
                    <h4>Data Collection & Preprocessing</h4>
                    <p>
                        Collected raw IMU data (accelerometer, gyroscope, magnetometer) at 100Hz sampling rate. 
                        Applied noise filtering, calibration corrections, and normalization techniques to prepare 
                        data for neural network training.
                    </p>
                </div>
                <div class="timeline-step">
                    <span class="step-number">2</span>
                    <h4>Feature Engineering</h4>
                    <p>
                        Designed temporal features including rolling statistics, differential values, and quaternion 
                        representations. Created sliding windows of sensor data to capture motion dynamics over time.
                    </p>
                </div>
                <div class="timeline-step">
                    <span class="step-number">3</span>
                    <h4>Model Architecture Design</h4>
                    <p>
                        Developed hybrid LSTM-GRU architecture with bidirectional layers to capture both past and 
                        future context. Implemented attention mechanisms to focus on critical time steps during orientation changes.
                    </p>
                </div>
                <div class="timeline-step">
                    <span class="step-number">4</span>
                    <h4>Training & Optimization</h4>
                    <p>
                        Trained models using Adam optimizer with custom loss function combining MSE and angular 
                        distance metrics. Applied dropout, batch normalization, and early stopping to prevent overfitting.
                    </p>
                </div>
                <div class="timeline-step">
                    <span class="step-number">5</span>
                    <h4>Evaluation & Validation</h4>
                    <p>
                        Conducted extensive testing across diverse motion patterns including rotations, translations,
                        and complex 3D movements. Compared performance against traditional Kalman filters and validated 
                        results using ground truth data from motion capture systems.
                    </p>
                </div>
                <div class="timeline-step">
                    <span class="step-number">6</span>
                    <h4>Real-Time Deployment</h4>
                    <p>
                        Optimized model for inference speed using TensorFlow Lite and quantization techniques. 
                        Deployed on embedded systems with real-time streaming capabilities for practical applications.
                    </p>
                </div>
            </div>
        </section>

        <!-- Results Section -->
        <section class="content-section">
            <h2 class="section-title">
                <i class="fas fa-chart-bar"></i>
                Results & Achievements
            </h2>
            <div class="results-grid">
                <div class="result-card">
                    <div class="result-number">0.2Â°</div>
                    <div class="result-label">Mean Prediction Error</div>
                </div>
                <div class="result-card">
                    <div class="result-number">98.7%</div>
                    <div class="result-label">Overall Accuracy</div>
                </div>
                <div class="result-card">
                    <div class="result-number">87ms</div>
                    <div class="result-label">Average Inference Time</div>
                </div>
                <div class="result-card">
                    <div class="result-number">40%</div>
                    <div class="result-label">Improvement Over Kalman</div>
                </div>
            </div>

            <div class="overview-text" style="margin-top: 2rem;">
                <p>
                    The AI-based orientation tracking system demonstrated <span class="highlight">significant performance 
                    improvements</span> over traditional Kalman filter approaches across all evaluation metrics. The deep 
                    learning model achieved a mean absolute error of just 0.2Â° in orientation estimation, representing a 
                    40% improvement in accuracy compared to the baseline Extended Kalman Filter (EKF).
                </p>
                <p>
                    Real-time performance was maintained with average inference times of 87ms, well within the 100ms 
                    requirement for responsive control systems. The model demonstrated exceptional robustness to sensor 
                    noise and drift, maintaining accuracy even during rapid acceleration changes and complex 3D rotations.
                </p>
            </div>
        </section>

        <!-- Code Sample -->
        <section class="content-section">
            <h2 class="section-title">
                <i class="fas fa-code"></i>
                Code Implementation
            </h2>
            <div class="code-section">
                <div class="code-header">
                    <span class="code-title">LSTM-GRU Model Architecture</span>
                    <button class="copy-btn" onclick="copyCode()">
                        <i class="fas fa-copy"></i> Copy Code
                    </button>
                </div>
                <pre><code id="codeBlock">import tensorflow as tf
from tensorflow.keras import layers, models

def build_orientation_model(sequence_length=50, n_features=9):
    """
    Build hybrid LSTM-GRU model for 3D orientation tracking
    
    Args:
        sequence_length: Number of time steps in input sequence
        n_features: Number of sensor features (3 accel + 3 gyro + 3 mag)
    
    Returns:
        Compiled Keras model
    """
    model = models.Sequential([
        # Input layer
        layers.Input(shape=(sequence_length, n_features)),
        
        # Bidirectional LSTM layer
        layers.Bidirectional(
            layers.LSTM(128, return_sequences=True, dropout=0.2)
        ),
        layers.BatchNormalization(),
        
        # GRU layer with attention
        layers.Bidirectional(
            layers.GRU(64, return_sequences=True, dropout=0.2)
        ),
        layers.BatchNormalization(),
        
        # Attention mechanism
        layers.Attention(),
        
        # Dense layers for orientation prediction
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.2),
        
        # Output: quaternion representation (4 values)
        layers.Dense(4, activation='tanh')
    ])
    
    # Compile with custom loss function
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='mse',
        metrics=['mae', 'mape']
    )
    
    return model

# Initialize and train model
model = build_orientation_model()
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=100,
    batch_size=32,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(patience=10),
        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
    ]
)</code></pre>
            </div>
        </section>

        <!-- Challenges Section -->
        <section class="content-section">
            <h2 class="section-title">
                <i class="fas fa-exclamation-triangle"></i>
                Challenges & Solutions
            </h2>
            <div class="challenges-list">
                <div class="challenge-item">
                    <div class="challenge-title">
                        <i class="fas fa-times-circle"></i> Challenge: Gimbal Lock in Euler Angles
                    </div>
                    <p>
                        Traditional Euler angle representations suffer from gimbal lock, causing singularities 
                        at certain orientations and making training unstable.
                    </p>
                    <div class="solution-title">
                        <i class="fas fa-check-circle"></i> Solution
                    </div>
                    <p>
                        Switched to quaternion representation for orientation, which provides smooth, continuous 
                        parameterization without singularities. Implemented custom normalization layers to maintain 
                        unit quaternion constraints during training.
                    </p>
                </div>

                <div class="challenge-item">
                    <div class="challenge-title">
                        <i class="fas fa-times-circle"></i> Challenge: Real-Time Performance Requirements
                    </div>
                    <p>
                        Initial model architecture was too complex for real-time inference, with latencies 
                        exceeding 200ms per prediction.
                    </p>
                    <div class="solution-title">
                        <i class="fas fa-check-circle"></i> Solution
                    </div>
                    <p>
                        Applied model optimization techniques including pruning, quantization, and TensorFlow Lite 
                        conversion. Reduced model size by 60% while maintaining 98% of original accuracy, achieving 
                        sub-100ms inference times.
                    </p>
                </div>

                <div class="challenge-item">
                    <div class="challenge-title">
                        <i class="fas fa-times-circle"></i> Challenge: Sensor Drift and Noise
                    </div>
                    <p>
                        Raw IMU data contains significant noise and drift, especially in magnetometer readings, 
                        leading to degraded orientation estimates over time.
                    </p>
                    <div class="solution-title">
                        <i class="fas fa-check-circle"></i> Solution
                    </div>
                    <p>
                        Implemented comprehensive preprocessing pipeline with adaptive filtering, bias correction, 
                        and data augmentation. Trained model on both clean and noisy data to improve robustness. 
                        Added LSTM memory cells to learn and compensate for systematic drift patterns.
                    </p>
                </div>

                <div class="challenge-item">
                    <div class="challenge-title">
                        <i class="fas fa-times-circle"></i> Challenge: Limited Training Data
                    </div>
                    <p>
                        Collecting diverse, labeled IMU data with ground truth orientation is time-consuming 
                        and requires expensive motion capture equipment.
                    </p>
                    <div class="solution-title">
                        <i class="fas fa-check-circle"></i> Solution
                    </div>
                    <p>
                        Developed synthetic data generation pipeline using physics simulation to create realistic 
                        IMU readings for known orientation trajectories. Combined synthetic data with real-world 
                        samples using domain adaptation techniques, expanding training set by 10x.
                    </p>
                </div>
            </div>
        </section>

        <!-- Visualizations -->
        <section class="content-section">
            <h2 class="section-title">
                <i class="fas fa-images"></i>
                Visualizations
            </h2>
            <div class="image-gallery">
                <div class="gallery-item">
                    <div class="gallery-image">
                        <i class="fas fa-chart-line"></i>
                    </div>
                    <div class="gallery-caption">
                        Model Training Loss Curves - Convergence achieved after 80 epochs
                    </div>
                </div>
                <div class="gallery-item">
                    <div class="gallery-image">
                        <i class="fas fa-cube"></i>
                    </div>
                    <div class="gallery-caption">
                        3D Orientation Prediction vs Ground Truth Comparison
                    </div>
                </div>
                <div class="gallery-item">
                    <div class="gallery-image">
                        <i class="fas fa-wave-square"></i>
                    </div>
                    <div class="gallery-caption">
                        IMU Sensor Data Preprocessing Pipeline
                    </div>
                </div>
                <div class="gallery-item">
                    <div class="gallery-image">
                        <i class="fas fa-network-wired"></i>
                    </div>
                    <div class="gallery-caption">
                        Neural Network Architecture Visualization
                    </div>
                </div>
            </div>
            <p style="color: var(--text-muted); font-style: italic; text-align: center; margin-top: 1rem;">
                Note: Replace placeholder icons with actual project images and visualizations
            </p>
        </section>

        <!-- Future Work -->
        <section class="content-section">
            <h2 class="section-title">
                <i class="fas fa-lightbulb"></i>
                Future Enhancements
            </h2>
            <div class="features-grid">
                <div class="feature-card">
                    <div class="feature-icon">
                        <i class="fas fa-mobile-alt"></i>
                    </div>
                    <h3>Mobile Deployment</h3>
                    <p>
                        Port model to mobile devices using TensorFlow Lite, enabling on-device orientation 
                        tracking for AR/VR applications without cloud connectivity.
                    </p>
                </div>
                <div class="feature-card">
                    <div class="feature-icon">
                        <i class="fas fa-robot"></i>
                    </div>
                    <h3>Multi-Sensor Fusion</h3>
                    <p>
                        Integrate additional sensors (GPS, barometer, camera) for enhanced accuracy and 
                        position estimation in outdoor environments.
                    </p>
                </div>
                <div class="feature-card">
                    <div class="feature-icon">
                        <i class="fas fa-graduation-cap"></i>
                    </div>
                    <h3>Transfer Learning</h3>
                    <p>
                        Develop pre-trained models that can be fine-tuned for specific sensor types and 
                        applications with minimal additional data collection.
                    </p>
                </div>
            </div>
        </section>

        <!-- Impact Section -->
        <section class="content-section">
            <h2 class="section-title">
                <i class="fas fa-rocket"></i>
                Project Impact & Applications
            </h2>
            <div class="overview-text">
                <p>
                    This project demonstrates the <span class="highlight">viability of deep learning approaches</span> 
                    for critical sensor fusion tasks traditionally dominated by classical control theory. The results 
                    have implications for multiple industries:
                </p>
                <ul style="color: var(--text-secondary); margin-left: 2rem; margin-top: 1rem; line-height: 2;">
                    <li><strong>Robotics:</strong> More accurate robot pose estimation for autonomous navigation and manipulation tasks</li>
                    <li><strong>Virtual Reality:</strong> Improved head tracking with reduced latency and drift for immersive experiences</li>
                    <li><strong>Aerospace:</strong> Enhanced attitude determination systems for drones and aircraft</li>
                    <li><strong>Wearables:</strong> Better motion tracking for fitness devices and medical monitoring applications</li>
                    <li><strong>Autonomous Vehicles:</strong> Complementary orientation sensing for GPS-denied environments</li>
                </ul>
                <p style="margin-top: 1.5rem;">
                    The project has been well-received in the academic community, with potential for publication in 
                    robotics and sensor fusion conferences. The open-source implementation serves as a foundation for 
                    future research in learned sensor fusion approaches.
                </p>
            </div>
        </section>

        <!-- Call to Action -->
        <section class="content-section">
            <div style="background: linear-gradient(135deg, var(--gradient-start), var(--gradient-end)); 
                        padding: 3rem; border-radius: 12px; text-align: center; color: white;">
                <h2 style="margin-bottom: 1rem; font-size: 2rem;">Interested in This Project?</h2>
                <p style="margin-bottom: 2rem; font-size: 1.1rem; opacity: 0.9;">
                    I'm happy to discuss the technical details, methodology, and implementation. The complete source code 
                    is available upon request for serious inquiries.
                </p>
                <div class="action-buttons" style="justify-content: center;">
                    <a href="../index.html#contact" class="btn btn-primary">
                        <i class="fas fa-envelope"></i> Get in Touch
                    </a>
                </div>
            </div>
        </section>
    </div>

    <footer>
        <div class="container">
            <p>&copy; 2025 Hassan Nader. All rights reserved. | Built with passion for data science.</p>
        </div>
    </footer>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const htmlElement = document.documentElement;
        const currentTheme = localStorage.getItem('theme') || 'dark';
        htmlElement.setAttribute('data-theme', currentTheme);

        themeToggle.addEventListener('click', () => {
            const currentTheme = htmlElement.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            htmlElement.setAttribute('data-theme', newTheme);
            localStorage.setItem('theme', newTheme);
        });

        // Smooth Scroll
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });
    </script>
</body>
</html>
